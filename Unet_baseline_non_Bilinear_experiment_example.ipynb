{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSrA5GWRYsIQ",
        "outputId": "93ea7600-8835-4ef3-caa1-53e522a97be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "630NUdnAY0Tp",
        "outputId": "8f2a6f18-e2a9-4530-916d-9c93fbabd189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-attentive-time-warping'...\n",
            "remote: Enumerating objects: 440, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 440 (delta 107), reused 98 (delta 56), pack-reused 289\u001b[K\n",
            "Receiving objects: 100% (440/440), 73.48 KiB | 4.59 MiB/s, done.\n",
            "Resolving deltas: 100% (302/302), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --branch u-net-modification https://github.com/Open-topic/deep-attentive-time-warping.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm6mX6iQY3m_",
        "outputId": "a66d561c-c31b-4a38-a990-8880ca3bfb13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/369.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/369.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.8/369.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -r  /content/deep-attentive-time-warping/requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJQ5bM0nY5Ev",
        "outputId": "d54b06a1-e4be-4c9c-e42d-f2043764c679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUV1CLOMY6YY",
        "outputId": "7769182a-718f-486b-cb0b-96db189931f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping\n"
          ]
        }
      ],
      "source": [
        "%cd /content/deep-attentive-time-warping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOA6Dz9imWFg"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "pre_training_config_path = \"/content/deep-attentive-time-warping/code/conf/pre_training.yaml\"\n",
        "metric_learning_config_path = \"/content/deep-attentive-time-warping/code/conf/metric_learning.yaml\"\n",
        "def change_batch_size(size: int,mode: int):\n",
        "  if mode==0:\n",
        "    path = pre_training_config_path\n",
        "  else:\n",
        "    path = metric_learning_config_path\n",
        "  conf = OmegaConf.load(path)\n",
        "  conf.batch_size = size\n",
        "  # dumps to file\n",
        "  with open(path, \"w\") as f:\n",
        "    OmegaConf.save(conf, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB9jU-FApbGZ"
      },
      "source": [
        "# Dataset Adiac No:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id3SuevDY7qp"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training-find.py dataset.ID=1 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq4V1R7KnnVY"
      },
      "outputs": [],
      "source": [
        "change_batch_size(64,0)\n",
        "change_batch_size(64,1) # I guess, metric learning should use roughly the same amount of Vram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97NCaU_gnzIi",
        "outputId": "3a2d706e-6f90-4314-8ba4-56d9e286191c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training.py:30: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 02:14:38,221][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 02:14:38,225][__main__][INFO] - dataset ID: 1, dataset name: Adiac\n",
            "[2023-11-15 02:14:38,226][__main__][INFO] - Number of training + validation data: 390\n",
            "[2023-11-15 02:14:38,226][__main__][INFO] - Length of data: 176\n",
            "[2023-11-15 02:14:53,769][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 02:14:53,770][__main__][INFO] - Length of val_loader: 209\n",
            "[2023-11-15 02:14:53,770][__main__][INFO] - Batch size: 64\n",
            "[2023-11-15 02:14:53,789][__main__][INFO] - data: 2023.11.15.11.14.53\n",
            "[2023-11-15 02:14:53,789][__main__][INFO] - save_name: _2023.11.15.11.14.53_lr_0.0001\n",
            "100% 100/100 [00:25<00:00,  3.93it/s]\n",
            "100% 209/209 [00:18<00:00, 11.21it/s]\n",
            "[2023-11-15 02:15:40,358][__main__][INFO] - [1/10]-ptime: 44.12, train loss: 0.0035, val loss: 0.0023\n",
            "100% 100/100 [00:23<00:00,  4.27it/s]\n",
            "100% 209/209 [00:18<00:00, 11.17it/s]\n",
            "[2023-11-15 02:16:23,073][__main__][INFO] - [2/10]-ptime: 42.15, train loss: 0.0025, val loss: 0.0024\n",
            "100% 100/100 [00:23<00:00,  4.31it/s]\n",
            "100% 209/209 [00:18<00:00, 11.25it/s]\n",
            "[2023-11-15 02:17:05,696][__main__][INFO] - [3/10]-ptime: 41.76, train loss: 0.0019, val loss: 0.0016\n",
            "100% 100/100 [00:23<00:00,  4.29it/s]\n",
            "100% 209/209 [00:18<00:00, 11.26it/s]\n",
            "[2023-11-15 02:17:48,783][__main__][INFO] - [4/10]-ptime: 41.88, train loss: 0.0016, val loss: 0.0012\n",
            "100% 100/100 [00:23<00:00,  4.25it/s]\n",
            "100% 209/209 [00:18<00:00, 11.30it/s]\n",
            "[2023-11-15 02:18:31,522][__main__][INFO] - [5/10]-ptime: 42.03, train loss: 0.0015, val loss: 0.0019\n",
            "100% 100/100 [00:23<00:00,  4.32it/s]\n",
            "100% 209/209 [00:18<00:00, 11.19it/s]\n",
            "[2023-11-15 02:19:14,147][__main__][INFO] - [6/10]-ptime: 41.83, train loss: 0.0014, val loss: 0.0010\n",
            "100% 100/100 [00:23<00:00,  4.30it/s]\n",
            "100% 209/209 [00:18<00:00, 11.30it/s]\n",
            "[2023-11-15 02:19:56,727][__main__][INFO] - [7/10]-ptime: 41.73, train loss: 0.0013, val loss: 0.0015\n",
            "100% 100/100 [00:23<00:00,  4.21it/s]\n",
            "100% 209/209 [00:18<00:00, 11.17it/s]\n",
            "[2023-11-15 02:20:40,523][__main__][INFO] - [8/10]-ptime: 42.49, train loss: 0.0012, val loss: 0.0009\n",
            "100% 100/100 [00:23<00:00,  4.27it/s]\n",
            "100% 209/209 [00:18<00:00, 11.24it/s]\n",
            "[2023-11-15 02:21:23,122][__main__][INFO] - [9/10]-ptime: 41.98, train loss: 0.0012, val loss: 0.0009\n",
            "100% 100/100 [00:23<00:00,  4.31it/s]\n",
            "100% 209/209 [00:18<00:00, 11.28it/s]\n",
            "[2023-11-15 02:22:05,626][__main__][INFO] - [10/10]-ptime: 41.72, train loss: 0.0011, val loss: 0.0009\n",
            "CPU times: user 4.94 s, sys: 478 ms, total: 5.42 s\n",
            "Wall time: 7min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training.py dataset.ID=1 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY6wh65wvg4w",
        "outputId": "0aff1b61-4d2c-4e93-90fd-afcf31757d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-metric-learning.py:32: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='metric_learning')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 02:22:13,765][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 02:22:13,767][__main__][INFO] - dataset ID: 1, dataset name: Adiac\n",
            "[2023-11-15 02:22:13,767][__main__][INFO] - Number of training + validation data: 390\n",
            "[2023-11-15 02:22:13,767][__main__][INFO] - Length of data: 176\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 176, 176]             --\n",
            "├─UNet: 1-1                                        [1, 1, 176, 176]          --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 176, 176]         --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 176, 176]         38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 88, 88]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 88, 88]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 44, 44]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 44, 44]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 22, 22]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 22, 22]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 11, 11]         --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 11, 11]         14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 22, 22]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 22, 22]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 22, 22]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 44, 44]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 44, 44]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 44, 44]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 88, 88]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 88, 88]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 88, 88]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 176, 176]         --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 176, 176]         32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 176, 176]         110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 176, 176]          --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 176, 176]          65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 25.81\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 271.85\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 396.02\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 176, 176]             --\n",
            "├─UNet: 1-1                                        [1, 1, 176, 176]          --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 176, 176]         --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 176, 176]         38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 88, 88]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 88, 88]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 44, 44]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 44, 44]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 22, 22]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 22, 22]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 11, 11]         --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 11, 11]         14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 22, 22]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 22, 22]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 22, 22]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 44, 44]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 44, 44]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 44, 44]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 88, 88]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 88, 88]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 88, 88]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 176, 176]         --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 176, 176]         32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 176, 176]         110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 176, 176]          --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 176, 176]          65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 25.81\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 271.85\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 396.02\n",
            "====================================================================================================\n",
            "[2023-11-15 02:22:17,951][__main__][INFO] - pre-trained model loading...\n",
            "[2023-11-15 02:22:17,951][__main__][INFO] - pre-trained model: /content/deep-attentive-time-warping//result/001_Adiac/pre_training/Adiac_2023.11.15.11.14.53_lr_0.0001_ProposedModel_epoch_10_loss_0.0009.pkl\n",
            "[2023-11-15 02:22:18,265][__main__][INFO] - Length of train_loader: 500\n",
            "[2023-11-15 02:22:18,268][__main__][INFO] - Batch size: 64\n",
            "[2023-11-15 02:22:18,310][__main__][INFO] - data: 2023.11.15.11.22.18\n",
            "[2023-11-15 02:22:18,310][__main__][INFO] - save_name: _2023.11.15.11.22.18_lr_0.0001\n",
            "100% 500/500 [01:53<00:00,  4.41it/s]\n",
            "100% 38/38 [00:21<00:00,  1.74it/s]\n",
            "[2023-11-15 02:24:35,337][__main__][INFO] - [1/20]-ptime: 135.18, train loss: 0.1953, val loss: 0.2217, val ER: 0.4474\n",
            "100% 500/500 [01:52<00:00,  4.43it/s]\n",
            "100% 38/38 [00:22<00:00,  1.70it/s]\n",
            "[2023-11-15 02:26:51,919][__main__][INFO] - [2/20]-ptime: 135.17, train loss: 0.1417, val loss: 0.0309, val ER: 0.3421\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.68it/s]\n",
            "[2023-11-15 02:29:08,486][__main__][INFO] - [3/20]-ptime: 135.17, train loss: 0.1242, val loss: 0.0423, val ER: 0.2632\n",
            "100% 500/500 [01:52<00:00,  4.45it/s]\n",
            "100% 38/38 [00:23<00:00,  1.59it/s]\n",
            "[2023-11-15 02:31:26,616][__main__][INFO] - [4/20]-ptime: 136.31, train loss: 0.1122, val loss: 0.0578, val ER: 0.5000\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.72it/s]\n",
            "[2023-11-15 02:33:43,517][__main__][INFO] - [5/20]-ptime: 134.58, train loss: 0.1005, val loss: 0.0837, val ER: 0.2368\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:21<00:00,  1.73it/s]\n",
            "[2023-11-15 02:36:00,074][__main__][INFO] - [6/20]-ptime: 134.59, train loss: 0.0926, val loss: 0.0596, val ER: 0.3158\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:21<00:00,  1.73it/s]\n",
            "[2023-11-15 02:38:16,015][__main__][INFO] - [7/20]-ptime: 134.56, train loss: 0.0838, val loss: 0.0525, val ER: 0.2632\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.72it/s]\n",
            "[2023-11-15 02:40:32,094][__main__][INFO] - [8/20]-ptime: 134.83, train loss: 0.0741, val loss: 0.0676, val ER: 0.3421\n",
            "100% 500/500 [01:52<00:00,  4.43it/s]\n",
            "100% 38/38 [00:22<00:00,  1.71it/s]\n",
            "[2023-11-15 02:42:48,525][__main__][INFO] - [9/20]-ptime: 135.15, train loss: 0.0642, val loss: 0.0471, val ER: 0.3947\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.68it/s]\n",
            "[2023-11-15 02:45:04,948][__main__][INFO] - [10/20]-ptime: 135.24, train loss: 0.0579, val loss: 0.0421, val ER: 0.2895\n",
            "100% 500/500 [01:54<00:00,  4.38it/s]\n",
            "100% 38/38 [00:22<00:00,  1.66it/s]\n",
            "[2023-11-15 02:47:22,988][__main__][INFO] - [11/20]-ptime: 136.91, train loss: 0.0555, val loss: 0.0466, val ER: 0.3421\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.70it/s]\n",
            "[2023-11-15 02:49:40,026][__main__][INFO] - [12/20]-ptime: 135.11, train loss: 0.0470, val loss: 0.0429, val ER: 0.2895\n",
            "100% 500/500 [01:52<00:00,  4.43it/s]\n",
            "100% 38/38 [00:22<00:00,  1.69it/s]\n",
            "[2023-11-15 02:51:57,167][__main__][INFO] - [13/20]-ptime: 135.25, train loss: 0.0447, val loss: 0.0326, val ER: 0.2895\n",
            "100% 500/500 [01:52<00:00,  4.43it/s]\n",
            "100% 38/38 [00:22<00:00,  1.72it/s]\n",
            "[2023-11-15 02:54:14,096][__main__][INFO] - [14/20]-ptime: 134.93, train loss: 0.0413, val loss: 0.0315, val ER: 0.2632\n",
            "100% 500/500 [01:52<00:00,  4.43it/s]\n",
            "100% 38/38 [00:22<00:00,  1.70it/s]\n",
            "[2023-11-15 02:56:31,140][__main__][INFO] - [15/20]-ptime: 135.09, train loss: 0.0388, val loss: 0.0333, val ER: 0.3158\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.71it/s]\n",
            "[2023-11-15 02:58:48,185][__main__][INFO] - [16/20]-ptime: 134.81, train loss: 0.0359, val loss: 0.0355, val ER: 0.1842\n",
            "100% 500/500 [01:52<00:00,  4.43it/s]\n",
            "100% 38/38 [00:21<00:00,  1.74it/s]\n",
            "[2023-11-15 03:01:04,845][__main__][INFO] - [17/20]-ptime: 134.67, train loss: 0.0358, val loss: 0.0415, val ER: 0.2632\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:21<00:00,  1.74it/s]\n",
            "[2023-11-15 03:03:21,186][__main__][INFO] - [18/20]-ptime: 134.59, train loss: 0.0334, val loss: 0.0324, val ER: 0.2105\n",
            "100% 500/500 [01:53<00:00,  4.42it/s]\n",
            "100% 38/38 [00:22<00:00,  1.73it/s]\n",
            "[2023-11-15 03:05:37,453][__main__][INFO] - [19/20]-ptime: 135.12, train loss: 0.0323, val loss: 0.0269, val ER: 0.3158\n",
            "100% 500/500 [01:52<00:00,  4.44it/s]\n",
            "100% 38/38 [00:22<00:00,  1.71it/s]\n",
            "[2023-11-15 03:07:53,282][__main__][INFO] - [20/20]-ptime: 134.68, train loss: 0.0305, val loss: 0.0356, val ER: 0.3421\n",
            "[2023-11-15 03:07:53,283][__main__][INFO] - test model loading...\n",
            "[2023-11-15 03:07:53,283][__main__][INFO] - test model: /content/deep-attentive-time-warping//result/001_Adiac/metric_learning/Adiac_2023.11.15.11.22.18_lr_0.0001_ProposedModel_epoch_16_ER_0.1842.pkl\n",
            "100% 391/391 [03:50<00:00,  1.70it/s]\n",
            "[2023-11-15 03:11:43,496][__main__][INFO] - test loss: 0.0432, test ER: 0.2532\n",
            "CPU times: user 27.9 s, sys: 3.02 s, total: 30.9 s\n",
            "Wall time: 49min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-metric-learning.py dataset.ID=1 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQCmx5E9onjB"
      },
      "source": [
        "# Dataset CricketY No:13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sQ8edOKorfT",
        "outputId": "7706fa1c-d79c-41b6-9b81-e7ade01d24b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training-find.py:26: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 03:11:51,325][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 03:11:51,327][__main__][INFO] - dataset ID: 13, dataset name: CricketY\n",
            "[2023-11-15 03:11:51,327][__main__][INFO] - Number of training + validation data: 390\n",
            "[2023-11-15 03:11:51,328][__main__][INFO] - Length of data: 300\n",
            "[2023-11-15 03:11:54,508][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 03:11:54,510][__main__][INFO] - Length of val_loader: 209\n",
            "[2023-11-15 03:11:54,510][__main__][INFO] - Batch size: 64\n",
            "[2023-11-15 03:11:54,523][__main__][INFO] - data: 2023.11.15.12.11.54\n",
            "[2023-11-15 03:11:54,523][__main__][INFO] - save_name: _2023.11.15.12.11.54_lr_0.0001\n",
            "data_shape [1, 300, 1]\n",
            "path_shape [1, 300, 300]\n",
            "tried power = 0\n",
            "check point\n",
            "data_shape [2, 300, 1]\n",
            "path_shape [2, 300, 300]\n",
            "tried power = 1\n",
            "check point\n",
            "data_shape [4, 300, 1]\n",
            "path_shape [4, 300, 300]\n",
            "tried power = 2\n",
            "check point\n",
            "data_shape [8, 300, 1]\n",
            "path_shape [8, 300, 300]\n",
            "tried power = 3\n",
            "check point\n",
            "data_shape [16, 300, 1]\n",
            "path_shape [16, 300, 300]\n",
            "tried power = 4\n",
            "check point\n",
            "data_shape [32, 300, 1]\n",
            "path_shape [32, 300, 300]\n",
            "tried power = 5\n",
            "check point\n",
            "data_shape [64, 300, 1]\n",
            "path_shape [64, 300, 300]\n",
            "tried power = 6\n",
            "check point\n",
            "CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 262.12 MiB is free. Process 271293 has 15.51 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 275.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "power:  5\n",
            "CPU times: user 103 ms, sys: 12.1 ms, total: 115 ms\n",
            "Wall time: 13.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training-find.py dataset.ID=13 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDSZAKasouWB"
      },
      "outputs": [],
      "source": [
        "change_batch_size(32,0)\n",
        "change_batch_size(32,1) # I guess, metric learning should use roughly the same amount of Vram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yjcHnCwowDA",
        "outputId": "54e4b00f-c6b3-4fd9-c229-24bfe1f88039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training.py:30: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 03:12:20,569][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 03:12:20,571][__main__][INFO] - dataset ID: 13, dataset name: CricketY\n",
            "[2023-11-15 03:12:20,572][__main__][INFO] - Number of training + validation data: 390\n",
            "[2023-11-15 03:12:20,572][__main__][INFO] - Length of data: 300\n",
            "[2023-11-15 03:12:23,736][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 03:12:23,736][__main__][INFO] - Length of val_loader: 418\n",
            "[2023-11-15 03:12:23,737][__main__][INFO] - Batch size: 32\n",
            "[2023-11-15 03:12:23,749][__main__][INFO] - data: 2023.11.15.12.12.23\n",
            "[2023-11-15 03:12:23,749][__main__][INFO] - save_name: _2023.11.15.12.12.23_lr_0.0001\n",
            "100% 100/100 [00:33<00:00,  3.02it/s]\n",
            "100% 418/418 [00:49<00:00,  8.40it/s]\n",
            "[2023-11-15 03:13:48,844][__main__][INFO] - [1/10]-ptime: 82.85, train loss: 0.0025, val loss: 0.0025\n",
            "100% 100/100 [00:32<00:00,  3.05it/s]\n",
            "100% 418/418 [00:51<00:00,  8.17it/s]\n",
            "[2023-11-15 03:15:14,132][__main__][INFO] - [2/10]-ptime: 84.02, train loss: 0.0023, val loss: 0.0022\n",
            "100% 100/100 [00:33<00:00,  3.03it/s]\n",
            "100% 418/418 [00:50<00:00,  8.20it/s]\n",
            "[2023-11-15 03:16:39,606][__main__][INFO] - [3/10]-ptime: 84.00, train loss: 0.0021, val loss: 0.0021\n",
            "100% 100/100 [00:32<00:00,  3.04it/s]\n",
            "100% 418/418 [00:50<00:00,  8.36it/s]\n",
            "[2023-11-15 03:18:03,322][__main__][INFO] - [4/10]-ptime: 82.91, train loss: 0.0021, val loss: 0.0021\n",
            "100% 100/100 [00:32<00:00,  3.04it/s]\n",
            "100% 418/418 [00:50<00:00,  8.31it/s]\n",
            "[2023-11-15 03:19:27,858][__main__][INFO] - [5/10]-ptime: 83.20, train loss: 0.0020, val loss: 0.0021\n",
            "100% 100/100 [00:32<00:00,  3.05it/s]\n",
            "100% 418/418 [00:50<00:00,  8.34it/s]\n",
            "[2023-11-15 03:20:51,503][__main__][INFO] - [6/10]-ptime: 82.84, train loss: 0.0020, val loss: 0.0021\n",
            "100% 100/100 [00:32<00:00,  3.04it/s]\n",
            "100% 418/418 [00:51<00:00,  8.15it/s]\n",
            "[2023-11-15 03:22:16,513][__main__][INFO] - [7/10]-ptime: 84.18, train loss: 0.0020, val loss: 0.0020\n",
            "100% 100/100 [00:32<00:00,  3.06it/s]\n",
            "100% 418/418 [00:49<00:00,  8.42it/s]\n",
            "[2023-11-15 03:23:39,937][__main__][INFO] - [8/10]-ptime: 82.36, train loss: 0.0020, val loss: 0.0020\n",
            "100% 100/100 [00:32<00:00,  3.04it/s]\n",
            "100% 418/418 [00:50<00:00,  8.28it/s]\n",
            "[2023-11-15 03:25:04,149][__main__][INFO] - [9/10]-ptime: 83.33, train loss: 0.0020, val loss: 0.0020\n",
            "100% 100/100 [00:32<00:00,  3.06it/s]\n",
            "100% 418/418 [00:49<00:00,  8.46it/s]\n",
            "[2023-11-15 03:26:27,248][__main__][INFO] - [10/10]-ptime: 82.16, train loss: 0.0019, val loss: 0.0020\n",
            "CPU times: user 10.1 s, sys: 1.01 s, total: 11.1 s\n",
            "Wall time: 14min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training.py dataset.ID=13 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94gfMjCyvrgu",
        "outputId": "9b775cc0-bd76-4aaa-c761-71466cfa4670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-metric-learning.py:32: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='metric_learning')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 03:26:33,827][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 03:26:33,829][__main__][INFO] - dataset ID: 13, dataset name: CricketY\n",
            "[2023-11-15 03:26:33,830][__main__][INFO] - Number of training + validation data: 390\n",
            "[2023-11-15 03:26:33,830][__main__][INFO] - Length of data: 300\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 300, 300]             --\n",
            "├─UNet: 1-1                                        [1, 1, 300, 300]          --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 300, 300]         --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 300, 300]         38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 150, 150]        --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 150, 150]        221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 75, 75]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 75, 75]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 37, 37]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 37, 37]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 18, 18]         --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 18, 18]         14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 37, 37]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 36, 36]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 37, 37]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 75, 75]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 74, 74]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 75, 75]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 150, 150]        --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 150, 150]        131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 150, 150]        443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 300, 300]         --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 300, 300]         32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 300, 300]         110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 300, 300]          --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 300, 300]          65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 73.90\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 786.96\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 911.13\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 300, 300]             --\n",
            "├─UNet: 1-1                                        [1, 1, 300, 300]          --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 300, 300]         --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 300, 300]         38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 150, 150]        --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 150, 150]        221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 75, 75]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 75, 75]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 37, 37]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 37, 37]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 18, 18]         --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 18, 18]         14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 37, 37]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 36, 36]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 37, 37]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 75, 75]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 74, 74]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 75, 75]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 150, 150]        --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 150, 150]        131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 150, 150]        443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 300, 300]         --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 300, 300]         32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 300, 300]         110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 300, 300]          --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 300, 300]          65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 73.90\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 786.96\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 911.13\n",
            "====================================================================================================\n",
            "[2023-11-15 03:26:37,038][__main__][INFO] - pre-trained model loading...\n",
            "[2023-11-15 03:26:37,038][__main__][INFO] - pre-trained model: /content/deep-attentive-time-warping//result/013_CricketY/pre_training/CricketY_2023.11.15.12.12.23_lr_0.0001_ProposedModel_epoch_9_loss_0.0020.pkl\n",
            "[2023-11-15 03:26:37,281][__main__][INFO] - Length of train_loader: 500\n",
            "[2023-11-15 03:26:37,281][__main__][INFO] - Batch size: 32\n",
            "[2023-11-15 03:26:37,296][__main__][INFO] - data: 2023.11.15.12.26.37\n",
            "[2023-11-15 03:26:37,296][__main__][INFO] - save_name: _2023.11.15.12.26.37_lr_0.0001\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:50<00:00,  1.34s/it]\n",
            "[2023-11-15 03:30:11,078][__main__][INFO] - [1/20]-ptime: 210.76, train loss: 0.2454, val loss: 0.1294, val ER: 0.1842\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.36s/it]\n",
            "[2023-11-15 03:33:43,673][__main__][INFO] - [2/20]-ptime: 211.23, train loss: 0.2172, val loss: 0.1292, val ER: 0.1842\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.34s/it]\n",
            "[2023-11-15 03:37:16,120][__main__][INFO] - [3/20]-ptime: 210.56, train loss: 0.1982, val loss: 0.1146, val ER: 0.2368\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 03:40:48,850][__main__][INFO] - [4/20]-ptime: 211.08, train loss: 0.1919, val loss: 0.1326, val ER: 0.1053\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.34s/it]\n",
            "[2023-11-15 03:44:20,759][__main__][INFO] - [5/20]-ptime: 210.63, train loss: 0.1670, val loss: 0.1455, val ER: 0.2105\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 03:47:53,788][__main__][INFO] - [6/20]-ptime: 210.89, train loss: 0.1431, val loss: 0.0879, val ER: 0.3421\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.34s/it]\n",
            "[2023-11-15 03:51:25,826][__main__][INFO] - [7/20]-ptime: 210.83, train loss: 0.1328, val loss: 0.1117, val ER: 0.1842\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.34s/it]\n",
            "[2023-11-15 03:54:57,551][__main__][INFO] - [8/20]-ptime: 210.53, train loss: 0.1205, val loss: 0.1004, val ER: 0.1842\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 03:58:30,272][__main__][INFO] - [9/20]-ptime: 210.87, train loss: 0.1051, val loss: 0.0832, val ER: 0.2632\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.34s/it]\n",
            "[2023-11-15 04:02:01,937][__main__][INFO] - [10/20]-ptime: 210.56, train loss: 0.0987, val loss: 0.1136, val ER: 0.2105\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.34s/it]\n",
            "[2023-11-15 04:05:34,435][__main__][INFO] - [11/20]-ptime: 210.63, train loss: 0.0902, val loss: 0.1063, val ER: 0.1316\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 04:09:07,005][__main__][INFO] - [12/20]-ptime: 210.81, train loss: 0.0835, val loss: 0.0762, val ER: 0.1579\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:50<00:00,  1.34s/it]\n",
            "[2023-11-15 04:12:38,741][__main__][INFO] - [13/20]-ptime: 210.58, train loss: 0.0751, val loss: 0.1049, val ER: 0.2105\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 04:16:11,247][__main__][INFO] - [14/20]-ptime: 210.67, train loss: 0.0688, val loss: 0.1076, val ER: 0.2368\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.36s/it]\n",
            "[2023-11-15 04:19:44,380][__main__][INFO] - [15/20]-ptime: 211.22, train loss: 0.0619, val loss: 0.0709, val ER: 0.2368\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 04:23:16,545][__main__][INFO] - [16/20]-ptime: 210.72, train loss: 0.0658, val loss: 0.0817, val ER: 0.1053\n",
            "100% 500/500 [02:39<00:00,  3.13it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 04:26:49,404][__main__][INFO] - [17/20]-ptime: 210.78, train loss: 0.0598, val loss: 0.0802, val ER: 0.2105\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 04:30:21,246][__main__][INFO] - [18/20]-ptime: 210.72, train loss: 0.0631, val loss: 0.0729, val ER: 0.1842\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.35s/it]\n",
            "[2023-11-15 04:33:53,599][__main__][INFO] - [19/20]-ptime: 210.70, train loss: 0.0584, val loss: 0.0789, val ER: 0.1579\n",
            "100% 500/500 [02:39<00:00,  3.14it/s]\n",
            "100% 38/38 [00:51<00:00,  1.36s/it]\n",
            "[2023-11-15 04:37:26,324][__main__][INFO] - [20/20]-ptime: 211.18, train loss: 0.0557, val loss: 0.0793, val ER: 0.2368\n",
            "[2023-11-15 04:37:26,325][__main__][INFO] - test model loading...\n",
            "[2023-11-15 04:37:26,325][__main__][INFO] - test model: /content/deep-attentive-time-warping//result/013_CricketY/metric_learning/CricketY_2023.11.15.12.26.37_lr_0.0001_ProposedModel_epoch_16_ER_0.1053.pkl\n",
            "100% 390/390 [08:46<00:00,  1.35s/it]\n",
            "[2023-11-15 04:46:12,617][__main__][INFO] - test loss: 0.0857, test ER: 0.2282\n",
            "CPU times: user 42.4 s, sys: 4.36 s, total: 46.7 s\n",
            "Wall time: 1h 19min 45s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-metric-learning.py dataset.ID=13 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpUtns8v-Oui"
      },
      "source": [
        "# Saving a checkpoint into Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDrPnyxtTrwA",
        "outputId": "8f647215-34f3-4cd4-998d-ef5690b9ded3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "475M\t/content/deep-attentive-time-warping/result\n"
          ]
        }
      ],
      "source": [
        "!du -sh /content/deep-attentive-time-warping/result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItLZlr7N-Rzd",
        "outputId": "9c4e9697-c0c9-4108-e7aa-ae25b7fd153a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "cp: -r not specified; omitting directory '/content/deep-attentive-time-warping/result'\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive\n",
        "!mkdir non_bilinear_result\n",
        "%cp /content/deep-attentive-time-warping/result /content/drive/MyDrive/bilinear_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXu1KhZST5Tv"
      },
      "outputs": [],
      "source": [
        "%cp /content/deep-attentive-time-warping/result /content/drive/MyDrive/bilinear_result -r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvVexQ5mpfvC"
      },
      "source": [
        "# Dataset DistalPhalanxTW No:18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI7ugIuyqDUg",
        "outputId": "6b2a1390-ca41-4e88-d64e-69883068e711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training-find.py:26: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 04:46:23,278][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 04:46:23,280][__main__][INFO] - dataset ID: 18, dataset name: DistalPhalanxTW\n",
            "[2023-11-15 04:46:23,280][__main__][INFO] - Number of training + validation data: 400\n",
            "[2023-11-15 04:46:23,281][__main__][INFO] - Length of data: 80\n",
            "[2023-11-15 04:46:27,663][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 04:46:27,664][__main__][INFO] - Length of val_loader: 440\n",
            "[2023-11-15 04:46:27,664][__main__][INFO] - Batch size: 32\n",
            "[2023-11-15 04:46:27,690][__main__][INFO] - data: 2023.11.15.13.46.27\n",
            "[2023-11-15 04:46:27,690][__main__][INFO] - save_name: _2023.11.15.13.46.27_lr_0.0001\n",
            "data_shape [1, 80, 1]\n",
            "path_shape [1, 80, 80]\n",
            "tried power = 0\n",
            "check point\n",
            "data_shape [2, 80, 1]\n",
            "path_shape [2, 80, 80]\n",
            "tried power = 1\n",
            "check point\n",
            "data_shape [4, 80, 1]\n",
            "path_shape [4, 80, 80]\n",
            "tried power = 2\n",
            "check point\n",
            "data_shape [8, 80, 1]\n",
            "path_shape [8, 80, 80]\n",
            "tried power = 3\n",
            "check point\n",
            "data_shape [16, 80, 1]\n",
            "path_shape [16, 80, 80]\n",
            "tried power = 4\n",
            "check point\n",
            "data_shape [32, 80, 1]\n",
            "path_shape [32, 80, 80]\n",
            "tried power = 5\n",
            "check point\n",
            "data_shape [64, 80, 1]\n",
            "path_shape [64, 80, 80]\n",
            "tried power = 6\n",
            "check point\n",
            "data_shape [128, 80, 1]\n",
            "path_shape [128, 80, 80]\n",
            "tried power = 7\n",
            "check point\n",
            "data_shape [256, 80, 1]\n",
            "path_shape [256, 80, 80]\n",
            "tried power = 8\n",
            "check point\n",
            "data_shape [512, 80, 1]\n",
            "path_shape [512, 80, 80]\n",
            "tried power = 9\n",
            "check point\n",
            "data_shape [1024, 80, 1]\n",
            "path_shape [1024, 80, 80]\n",
            "tried power = 10\n",
            "check point\n",
            "CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 74.12 MiB is free. Process 657968 has 15.70 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 112.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "power:  9\n",
            "CPU times: user 115 ms, sys: 15.4 ms, total: 130 ms\n",
            "Wall time: 14.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training-find.py dataset.ID=18 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk-Za6PqqDUi"
      },
      "outputs": [],
      "source": [
        "change_batch_size(512,0)\n",
        "change_batch_size(512,1) # I guess, metric learning should use roughly the same amount of Vram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bszWDYUnqDUj",
        "outputId": "8acea582-d2d6-4c36-8c0d-f08757d2a3ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training.py:30: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 10:39:23,580][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 10:39:23,585][__main__][INFO] - dataset ID: 18, dataset name: DistalPhalanxTW\n",
            "[2023-11-15 10:39:23,585][__main__][INFO] - Number of training + validation data: 400\n",
            "[2023-11-15 10:39:23,585][__main__][INFO] - Length of data: 80\n",
            "[2023-11-15 10:39:37,717][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 10:39:37,718][__main__][INFO] - Length of val_loader: 28\n",
            "[2023-11-15 10:39:37,718][__main__][INFO] - Batch size: 512\n",
            "[2023-11-15 10:39:37,743][__main__][INFO] - data: 2023.11.15.19.39.37\n",
            "[2023-11-15 10:39:37,743][__main__][INFO] - save_name: _2023.11.15.19.39.37_lr_0.0001\n",
            "100% 100/100 [00:41<00:00,  2.39it/s]\n",
            "100% 28/28 [00:12<00:00,  2.23it/s]\n",
            "[2023-11-15 10:40:34,053][__main__][INFO] - [1/10]-ptime: 54.50, train loss: 0.0051, val loss: 0.0033\n",
            "100% 100/100 [00:41<00:00,  2.39it/s]\n",
            "100% 28/28 [00:05<00:00,  4.79it/s]\n",
            "[2023-11-15 10:41:22,589][__main__][INFO] - [2/10]-ptime: 47.73, train loss: 0.0027, val loss: 0.0024\n",
            "100% 100/100 [00:40<00:00,  2.48it/s]\n",
            "100% 28/28 [00:08<00:00,  3.12it/s]\n",
            "[2023-11-15 10:42:12,794][__main__][INFO] - [3/10]-ptime: 49.32, train loss: 0.0020, val loss: 0.0022\n",
            "100% 100/100 [00:40<00:00,  2.49it/s]\n",
            "100% 28/28 [00:06<00:00,  4.59it/s]\n",
            "[2023-11-15 10:42:59,811][__main__][INFO] - [4/10]-ptime: 46.18, train loss: 0.0018, val loss: 0.0020\n",
            "100% 100/100 [00:40<00:00,  2.48it/s]\n",
            "100% 28/28 [00:09<00:00,  3.04it/s]\n",
            "[2023-11-15 10:43:49,999][__main__][INFO] - [5/10]-ptime: 49.56, train loss: 0.0017, val loss: 0.0021\n",
            "100% 100/100 [00:39<00:00,  2.51it/s]\n",
            "100% 28/28 [00:06<00:00,  4.66it/s]\n",
            "[2023-11-15 10:44:36,458][__main__][INFO] - [6/10]-ptime: 45.88, train loss: 0.0015, val loss: 0.0021\n",
            "100% 100/100 [00:40<00:00,  2.50it/s]\n",
            "100% 28/28 [00:09<00:00,  3.11it/s]\n",
            "[2023-11-15 10:45:26,915][__main__][INFO] - [7/10]-ptime: 49.04, train loss: 0.0015, val loss: 0.0020\n",
            "100% 100/100 [00:40<00:00,  2.50it/s]\n",
            "100% 28/28 [00:06<00:00,  4.61it/s]\n",
            "[2023-11-15 10:46:13,853][__main__][INFO] - [8/10]-ptime: 46.09, train loss: 0.0014, val loss: 0.0019\n",
            "100% 100/100 [00:40<00:00,  2.49it/s]\n",
            "100% 28/28 [00:08<00:00,  3.27it/s]\n",
            "[2023-11-15 10:47:03,527][__main__][INFO] - [9/10]-ptime: 48.66, train loss: 0.0013, val loss: 0.0020\n",
            "100% 100/100 [00:41<00:00,  2.42it/s]\n",
            "100% 28/28 [00:07<00:00,  3.96it/s]\n",
            "[2023-11-15 10:47:52,790][__main__][INFO] - [10/10]-ptime: 48.46, train loss: 0.0013, val loss: 0.0019\n",
            "CPU times: user 5.14 s, sys: 591 ms, total: 5.73 s\n",
            "Wall time: 8min 39s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training.py dataset.ID=18 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfni2hFavs19",
        "outputId": "027eeee8-e044-45e6-c86b-17394f0c168d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-metric-learning.py:32: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='metric_learning')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 10:48:00,306][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 10:48:00,310][__main__][INFO] - dataset ID: 18, dataset name: DistalPhalanxTW\n",
            "[2023-11-15 10:48:00,310][__main__][INFO] - Number of training + validation data: 400\n",
            "[2023-11-15 10:48:00,311][__main__][INFO] - Length of data: 80\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 80, 80]               --\n",
            "├─UNet: 1-1                                        [1, 1, 80, 80]            --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 80, 80]           --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 80, 80]           38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 40, 40]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 40, 40]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 20, 20]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 20, 20]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 10, 10]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 10, 10]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 5, 5]           --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 5, 5]           14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 10, 10]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 10, 10]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 10, 10]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 20, 20]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 20, 20]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 20, 20]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 40, 40]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 40, 40]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 40, 40]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 80, 80]           --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 80, 80]           32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 80, 80]           110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 80, 80]            --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 80, 80]            65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 5.33\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 56.17\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 180.34\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 80, 80]               --\n",
            "├─UNet: 1-1                                        [1, 1, 80, 80]            --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 80, 80]           --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 80, 80]           38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 40, 40]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 40, 40]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 20, 20]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 20, 20]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 10, 10]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 10, 10]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 5, 5]           --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 5, 5]           14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 10, 10]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 10, 10]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 10, 10]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 20, 20]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 20, 20]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 20, 20]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 40, 40]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 40, 40]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 40, 40]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 80, 80]           --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 80, 80]           32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 80, 80]           110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 80, 80]            --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 80, 80]            65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 5.33\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 56.17\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 180.34\n",
            "====================================================================================================\n",
            "[2023-11-15 10:48:07,252][__main__][INFO] - pre-trained model loading...\n",
            "[2023-11-15 10:48:07,252][__main__][INFO] - pre-trained model: /content/deep-attentive-time-warping//result/018_DistalPhalanxTW/pre_training/DistalPhalanxTW_2023.11.15.19.39.37_lr_0.0001_ProposedModel_epoch_10_loss_0.0019.pkl\n",
            "[2023-11-15 10:48:07,545][__main__][INFO] - Length of train_loader: 500\n",
            "[2023-11-15 10:48:07,546][__main__][INFO] - Batch size: 512\n",
            "[2023-11-15 10:48:07,561][__main__][INFO] - data: 2023.11.15.19.48.07\n",
            "[2023-11-15 10:48:07,561][__main__][INFO] - save_name: _2023.11.15.19.48.07_lr_0.0001\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:10<00:00,  3.75it/s]\n",
            "[2023-11-15 10:51:26,672][__main__][INFO] - [1/20]-ptime: 197.13, train loss: 0.0823, val loss: 0.0621, val ER: 0.3077\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:09<00:00,  3.93it/s]\n",
            "[2023-11-15 10:54:44,184][__main__][INFO] - [2/20]-ptime: 196.32, train loss: 0.0716, val loss: 0.1395, val ER: 0.3333\n",
            "100% 500/500 [03:06<00:00,  2.69it/s]\n",
            "100% 39/39 [00:09<00:00,  4.14it/s]\n",
            "[2023-11-15 10:58:02,076][__main__][INFO] - [3/20]-ptime: 195.60, train loss: 0.0709, val loss: 0.0622, val ER: 0.2821\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:10<00:00,  3.78it/s]\n",
            "[2023-11-15 11:01:20,984][__main__][INFO] - [4/20]-ptime: 196.80, train loss: 0.0716, val loss: 0.0615, val ER: 0.3077\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:11<00:00,  3.42it/s]\n",
            "[2023-11-15 11:04:40,022][__main__][INFO] - [5/20]-ptime: 197.77, train loss: 0.0693, val loss: 0.0661, val ER: 0.3077\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:10<00:00,  3.62it/s]\n",
            "[2023-11-15 11:07:58,664][__main__][INFO] - [6/20]-ptime: 197.29, train loss: 0.0363, val loss: 0.0717, val ER: 0.2821\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:09<00:00,  4.11it/s]\n",
            "[2023-11-15 11:11:16,571][__main__][INFO] - [7/20]-ptime: 195.84, train loss: 0.0251, val loss: 0.3124, val ER: 0.4359\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:10<00:00,  3.55it/s]\n",
            "[2023-11-15 11:14:35,455][__main__][INFO] - [8/20]-ptime: 197.23, train loss: 0.0256, val loss: 0.0733, val ER: 0.2308\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:10<00:00,  3.70it/s]\n",
            "[2023-11-15 11:17:53,897][__main__][INFO] - [9/20]-ptime: 196.96, train loss: 0.0213, val loss: 0.0687, val ER: 0.2051\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:10<00:00,  3.79it/s]\n",
            "[2023-11-15 11:21:11,542][__main__][INFO] - [10/20]-ptime: 196.57, train loss: 0.0175, val loss: 0.0768, val ER: 0.2308\n",
            "100% 500/500 [03:05<00:00,  2.69it/s]\n",
            "100% 39/39 [00:09<00:00,  3.90it/s]\n",
            "[2023-11-15 11:24:29,518][__main__][INFO] - [11/20]-ptime: 195.93, train loss: 0.0281, val loss: 0.0647, val ER: 0.1795\n",
            "100% 500/500 [03:05<00:00,  2.69it/s]\n",
            "100% 39/39 [00:10<00:00,  3.87it/s]\n",
            "[2023-11-15 11:27:47,402][__main__][INFO] - [12/20]-ptime: 195.98, train loss: 0.0246, val loss: 0.0656, val ER: 0.2051\n",
            "100% 500/500 [03:05<00:00,  2.70it/s]\n",
            "100% 39/39 [00:13<00:00,  2.92it/s]\n",
            "[2023-11-15 11:31:07,337][__main__][INFO] - [13/20]-ptime: 198.84, train loss: 0.0183, val loss: 0.0757, val ER: 0.2308\n",
            "100% 500/500 [03:05<00:00,  2.69it/s]\n",
            "100% 39/39 [00:09<00:00,  3.94it/s]\n",
            "[2023-11-15 11:34:24,152][__main__][INFO] - [14/20]-ptime: 195.71, train loss: 0.0172, val loss: 0.0713, val ER: 0.2308\n",
            "100% 500/500 [03:06<00:00,  2.69it/s]\n",
            "100% 39/39 [00:09<00:00,  4.06it/s]\n",
            "[2023-11-15 11:37:41,947][__main__][INFO] - [15/20]-ptime: 195.81, train loss: 0.0166, val loss: 0.0766, val ER: 0.2308\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:11<00:00,  3.47it/s]\n",
            "[2023-11-15 11:41:00,635][__main__][INFO] - [16/20]-ptime: 197.54, train loss: 0.0141, val loss: 0.0783, val ER: 0.2051\n",
            "100% 500/500 [03:05<00:00,  2.69it/s]\n",
            "100% 39/39 [00:11<00:00,  3.47it/s]\n",
            "[2023-11-15 11:44:19,004][__main__][INFO] - [17/20]-ptime: 197.19, train loss: 0.0112, val loss: 0.0955, val ER: 0.2564\n",
            "100% 500/500 [03:05<00:00,  2.69it/s]\n",
            "100% 39/39 [00:10<00:00,  3.80it/s]\n",
            "[2023-11-15 11:47:36,192][__main__][INFO] - [18/20]-ptime: 196.13, train loss: 0.0086, val loss: 0.0747, val ER: 0.2308\n",
            "100% 500/500 [03:05<00:00,  2.69it/s]\n",
            "100% 39/39 [00:09<00:00,  3.97it/s]\n",
            "[2023-11-15 11:50:52,893][__main__][INFO] - [19/20]-ptime: 195.63, train loss: 0.0063, val loss: 0.0705, val ER: 0.2308\n",
            "100% 500/500 [03:06<00:00,  2.68it/s]\n",
            "100% 39/39 [00:20<00:00,  1.87it/s]\n",
            "[2023-11-15 11:54:21,414][__main__][INFO] - [20/20]-ptime: 207.12, train loss: 0.0025, val loss: 0.0643, val ER: 0.2564\n",
            "[2023-11-15 11:54:21,415][__main__][INFO] - test model loading...\n",
            "[2023-11-15 11:54:21,415][__main__][INFO] - test model: /content/deep-attentive-time-warping//result/018_DistalPhalanxTW/metric_learning/DistalPhalanxTW_2023.11.15.19.48.07_lr_0.0001_ProposedModel_epoch_11_ER_0.1795.pkl\n",
            "100% 139/139 [00:39<00:00,  3.55it/s]\n",
            "[2023-11-15 11:55:00,704][__main__][INFO] - test loss: 0.0927, test ER: 0.3525\n",
            "CPU times: user 39.4 s, sys: 4.23 s, total: 43.6 s\n",
            "Wall time: 1h 7min 6s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-metric-learning.py dataset.ID=18 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80z4sQjHprtK"
      },
      "source": [
        "# Dataset ECG200 No: 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BERABY8xqD5c",
        "outputId": "0ec77d33-42df-4353-b4a8-b5c72a021f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training-find.py:26: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 04:46:47,205][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 04:46:47,207][__main__][INFO] - dataset ID: 20, dataset name: ECG200\n",
            "[2023-11-15 04:46:47,207][__main__][INFO] - Number of training + validation data: 100\n",
            "[2023-11-15 04:46:47,207][__main__][INFO] - Length of data: 96\n",
            "[2023-11-15 04:46:50,395][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 04:46:50,396][__main__][INFO] - Length of val_loader: 26\n",
            "[2023-11-15 04:46:50,396][__main__][INFO] - Batch size: 32\n",
            "[2023-11-15 04:46:50,410][__main__][INFO] - data: 2023.11.15.13.46.50\n",
            "[2023-11-15 04:46:50,410][__main__][INFO] - save_name: _2023.11.15.13.46.50_lr_0.0001\n",
            "data_shape [1, 96, 1]\n",
            "path_shape [1, 96, 96]\n",
            "tried power = 0\n",
            "check point\n",
            "data_shape [2, 96, 1]\n",
            "path_shape [2, 96, 96]\n",
            "tried power = 1\n",
            "check point\n",
            "data_shape [4, 96, 1]\n",
            "path_shape [4, 96, 96]\n",
            "tried power = 2\n",
            "check point\n",
            "data_shape [8, 96, 1]\n",
            "path_shape [8, 96, 96]\n",
            "tried power = 3\n",
            "check point\n",
            "data_shape [16, 96, 1]\n",
            "path_shape [16, 96, 96]\n",
            "tried power = 4\n",
            "check point\n",
            "data_shape [32, 96, 1]\n",
            "path_shape [32, 96, 96]\n",
            "tried power = 5\n",
            "check point\n",
            "data_shape [64, 96, 1]\n",
            "path_shape [64, 96, 96]\n",
            "tried power = 6\n",
            "check point\n",
            "data_shape [128, 96, 1]\n",
            "path_shape [128, 96, 96]\n",
            "tried power = 7\n",
            "check point\n",
            "data_shape [256, 96, 1]\n",
            "path_shape [256, 96, 96]\n",
            "tried power = 8\n",
            "check point\n",
            "data_shape [512, 96, 1]\n",
            "path_shape [512, 96, 96]\n",
            "tried power = 9\n",
            "check point\n",
            "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 15.77 GiB of which 692.12 MiB is free. Process 659190 has 15.09 GiB memory in use. Of the allocated memory 12.97 GiB is allocated by PyTorch, and 853.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "power:  8\n",
            "CPU times: user 108 ms, sys: 12.7 ms, total: 121 ms\n",
            "Wall time: 14 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training-find.py dataset.ID=20 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug4RDqPoqD5d"
      },
      "outputs": [],
      "source": [
        "power_batch_size = 2**8\n",
        "change_batch_size(power_batch_size,0)\n",
        "change_batch_size(power_batch_size,1) # I guess, metric learning should use roughly the same amount of Vram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n99VOIUbqD5e",
        "outputId": "4e9ffe5a-0dd6-492e-c4ae-054a2ffccf5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training.py:30: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 11:55:08,915][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 11:55:08,917][__main__][INFO] - dataset ID: 20, dataset name: ECG200\n",
            "[2023-11-15 11:55:08,917][__main__][INFO] - Number of training + validation data: 100\n",
            "[2023-11-15 11:55:08,917][__main__][INFO] - Length of data: 96\n",
            "[2023-11-15 11:55:13,189][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 11:55:13,191][__main__][INFO] - Length of val_loader: 4\n",
            "[2023-11-15 11:55:13,192][__main__][INFO] - Batch size: 256\n",
            "[2023-11-15 11:55:13,215][__main__][INFO] - data: 2023.11.15.20.55.13\n",
            "[2023-11-15 11:55:13,215][__main__][INFO] - save_name: _2023.11.15.20.55.13_lr_0.0001\n",
            "100% 100/100 [00:30<00:00,  3.26it/s]\n",
            "100% 4/4 [00:00<00:00,  4.07it/s]\n",
            "[2023-11-15 11:55:46,316][__main__][INFO] - [1/10]-ptime: 31.64, train loss: 0.0063, val loss: 0.0055\n",
            "100% 100/100 [00:28<00:00,  3.52it/s]\n",
            "100% 4/4 [00:01<00:00,  3.90it/s]\n",
            "[2023-11-15 11:56:16,965][__main__][INFO] - [2/10]-ptime: 29.41, train loss: 0.0042, val loss: 0.0053\n",
            "100% 100/100 [00:28<00:00,  3.47it/s]\n",
            "100% 4/4 [00:00<00:00,  5.34it/s]\n",
            "[2023-11-15 11:56:47,206][__main__][INFO] - [3/10]-ptime: 29.59, train loss: 0.0027, val loss: 0.0056\n",
            "100% 100/100 [00:28<00:00,  3.48it/s]\n",
            "100% 4/4 [00:01<00:00,  3.55it/s]\n",
            "[2023-11-15 11:57:17,822][__main__][INFO] - [4/10]-ptime: 29.84, train loss: 0.0015, val loss: 0.0055\n",
            "100% 100/100 [00:28<00:00,  3.49it/s]\n",
            "100% 4/4 [00:00<00:00,  5.32it/s]\n",
            "[2023-11-15 11:57:47,853][__main__][INFO] - [5/10]-ptime: 29.43, train loss: 0.0008, val loss: 0.0056\n",
            "100% 100/100 [00:29<00:00,  3.43it/s]\n",
            "100% 4/4 [00:00<00:00,  5.49it/s]\n",
            "[2023-11-15 11:58:18,284][__main__][INFO] - [6/10]-ptime: 29.89, train loss: 0.0005, val loss: 0.0057\n",
            "100% 100/100 [00:28<00:00,  3.54it/s]\n",
            "100% 4/4 [00:01<00:00,  3.57it/s]\n",
            "[2023-11-15 11:58:48,592][__main__][INFO] - [7/10]-ptime: 29.39, train loss: 0.0003, val loss: 0.0058\n",
            "100% 100/100 [00:28<00:00,  3.45it/s]\n",
            "100% 4/4 [00:00<00:00,  5.37it/s]\n",
            "[2023-11-15 11:59:18,916][__main__][INFO] - [8/10]-ptime: 29.70, train loss: 0.0002, val loss: 0.0056\n",
            "100% 100/100 [00:28<00:00,  3.50it/s]\n",
            "100% 4/4 [00:01<00:00,  3.63it/s]\n",
            "[2023-11-15 11:59:49,609][__main__][INFO] - [9/10]-ptime: 29.66, train loss: 0.0001, val loss: 0.0057\n",
            "100% 100/100 [00:28<00:00,  3.49it/s]\n",
            "100% 4/4 [00:00<00:00,  5.49it/s]\n",
            "[2023-11-15 12:00:19,516][__main__][INFO] - [10/10]-ptime: 29.38, train loss: 0.0001, val loss: 0.0056\n",
            "CPU times: user 3.33 s, sys: 371 ms, total: 3.7 s\n",
            "Wall time: 5min 16s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training.py dataset.ID=20 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jl9viH-vugf",
        "outputId": "092a4cfb-9735-4797-ac89-203141fc4260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-metric-learning.py:32: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='metric_learning')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 12:00:26,198][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 12:00:26,202][__main__][INFO] - dataset ID: 20, dataset name: ECG200\n",
            "[2023-11-15 12:00:26,202][__main__][INFO] - Number of training + validation data: 100\n",
            "[2023-11-15 12:00:26,202][__main__][INFO] - Length of data: 96\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 96, 96]               --\n",
            "├─UNet: 1-1                                        [1, 1, 96, 96]            --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 96, 96]           --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 96, 96]           38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 48, 48]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 48, 48]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 24, 24]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 24, 24]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 12, 12]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 12, 12]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 6, 6]           --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 6, 6]           14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 12, 12]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 12, 12]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 12, 12]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 24, 24]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 24, 24]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 24, 24]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 48, 48]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 48, 48]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 48, 48]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 96, 96]           --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 96, 96]           32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 96, 96]           110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 96, 96]            --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 96, 96]            65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 7.68\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 80.88\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 205.05\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 96, 96]               --\n",
            "├─UNet: 1-1                                        [1, 1, 96, 96]            --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 96, 96]           --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 96, 96]           38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 48, 48]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 48, 48]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 24, 24]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 24, 24]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 12, 12]          --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 12, 12]          3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 6, 6]           --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 6, 6]           14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 12, 12]          --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 12, 12]          2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 12, 12]          7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 24, 24]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 24, 24]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 24, 24]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 48, 48]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 48, 48]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 48, 48]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 96, 96]           --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 96, 96]           32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 96, 96]           110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 96, 96]            --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 96, 96]            65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 7.68\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 80.88\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 205.05\n",
            "====================================================================================================\n",
            "[2023-11-15 12:00:29,930][__main__][INFO] - pre-trained model loading...\n",
            "[2023-11-15 12:00:29,931][__main__][INFO] - pre-trained model: /content/deep-attentive-time-warping//result/020_ECG200/pre_training/ECG200_2023.11.15.20.55.13_lr_0.0001_ProposedModel_epoch_2_loss_0.0053.pkl\n",
            "[2023-11-15 12:00:30,162][__main__][INFO] - Length of train_loader: 500\n",
            "[2023-11-15 12:00:30,162][__main__][INFO] - Batch size: 256\n",
            "[2023-11-15 12:00:30,180][__main__][INFO] - data: 2023.11.15.21.00.30\n",
            "[2023-11-15 12:00:30,180][__main__][INFO] - save_name: _2023.11.15.21.00.30_lr_0.0001\n",
            "100% 500/500 [02:15<00:00,  3.70it/s]\n",
            "100% 9/9 [00:01<00:00,  4.63it/s]\n",
            "[2023-11-15 12:02:49,040][__main__][INFO] - [1/20]-ptime: 137.10, train loss: 0.0408, val loss: 0.1320, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  4.22it/s]\n",
            "[2023-11-15 12:05:07,075][__main__][INFO] - [2/20]-ptime: 136.66, train loss: 0.0043, val loss: 0.0637, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.71it/s]\n",
            "100% 9/9 [00:02<00:00,  4.17it/s]\n",
            "[2023-11-15 12:07:25,114][__main__][INFO] - [3/20]-ptime: 136.77, train loss: 0.0190, val loss: 0.0677, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.83it/s]\n",
            "[2023-11-15 12:09:44,151][__main__][INFO] - [4/20]-ptime: 136.71, train loss: 0.0035, val loss: 0.0592, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.87it/s]\n",
            "[2023-11-15 12:12:02,910][__main__][INFO] - [5/20]-ptime: 136.89, train loss: 0.0089, val loss: 0.1021, val ER: 0.1111\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.82it/s]\n",
            "[2023-11-15 12:14:21,691][__main__][INFO] - [6/20]-ptime: 136.73, train loss: 0.0033, val loss: 0.0609, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.88it/s]\n",
            "[2023-11-15 12:16:40,134][__main__][INFO] - [7/20]-ptime: 136.56, train loss: 0.0031, val loss: 0.0787, val ER: 0.1111\n",
            "100% 500/500 [02:14<00:00,  3.73it/s]\n",
            "100% 9/9 [00:02<00:00,  4.01it/s]\n",
            "[2023-11-15 12:18:58,590][__main__][INFO] - [8/20]-ptime: 136.44, train loss: 0.0028, val loss: 0.0510, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.82it/s]\n",
            "[2023-11-15 12:21:17,587][__main__][INFO] - [9/20]-ptime: 136.76, train loss: 0.0027, val loss: 0.0678, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.77it/s]\n",
            "[2023-11-15 12:23:36,368][__main__][INFO] - [10/20]-ptime: 136.75, train loss: 0.0027, val loss: 0.0553, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.81it/s]\n",
            "[2023-11-15 12:25:55,345][__main__][INFO] - [11/20]-ptime: 136.92, train loss: 0.0026, val loss: 0.0590, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.71it/s]\n",
            "100% 9/9 [00:02<00:00,  3.71it/s]\n",
            "[2023-11-15 12:28:14,830][__main__][INFO] - [12/20]-ptime: 137.15, train loss: 0.0026, val loss: 0.0527, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.71it/s]\n",
            "100% 9/9 [00:02<00:00,  3.80it/s]\n",
            "[2023-11-15 12:30:34,174][__main__][INFO] - [13/20]-ptime: 137.25, train loss: 0.0350, val loss: 0.0372, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.73it/s]\n",
            "100% 9/9 [00:02<00:00,  3.79it/s]\n",
            "[2023-11-15 12:32:53,006][__main__][INFO] - [14/20]-ptime: 136.59, train loss: 0.0033, val loss: 0.0576, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.73it/s]\n",
            "100% 9/9 [00:02<00:00,  3.60it/s]\n",
            "[2023-11-15 12:35:11,684][__main__][INFO] - [15/20]-ptime: 136.52, train loss: 0.0030, val loss: 0.0588, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.73it/s]\n",
            "100% 9/9 [00:02<00:00,  3.71it/s]\n",
            "[2023-11-15 12:37:30,445][__main__][INFO] - [16/20]-ptime: 136.47, train loss: 0.0043, val loss: 0.1230, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:01<00:00,  5.57it/s]\n",
            "[2023-11-15 12:39:47,887][__main__][INFO] - [17/20]-ptime: 136.02, train loss: 0.0031, val loss: 0.0378, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:01<00:00,  5.54it/s]\n",
            "[2023-11-15 12:42:05,155][__main__][INFO] - [18/20]-ptime: 135.97, train loss: 0.0027, val loss: 0.0341, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:01<00:00,  5.58it/s]\n",
            "[2023-11-15 12:44:22,304][__main__][INFO] - [19/20]-ptime: 135.86, train loss: 0.0027, val loss: 0.0421, val ER: 0.0000\n",
            "100% 500/500 [02:14<00:00,  3.72it/s]\n",
            "100% 9/9 [00:02<00:00,  3.73it/s]\n",
            "[2023-11-15 12:46:40,923][__main__][INFO] - [20/20]-ptime: 136.81, train loss: 0.0027, val loss: 0.0777, val ER: 0.0000\n",
            "[2023-11-15 12:46:40,924][__main__][INFO] - test model loading...\n",
            "[2023-11-15 12:46:40,924][__main__][INFO] - test model: /content/deep-attentive-time-warping//result/020_ECG200/metric_learning/ECG200_2023.11.15.21.00.30_lr_0.0001_ProposedModel_epoch_20_ER_0.0000.pkl\n",
            "100% 100/100 [00:21<00:00,  4.72it/s]\n",
            "[2023-11-15 12:47:02,237][__main__][INFO] - test loss: 0.1801, test ER: 0.1600\n",
            "CPU times: user 27.4 s, sys: 2.97 s, total: 30.3 s\n",
            "Wall time: 46min 42s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-metric-learning.py dataset.ID=20 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw0yhsKhpz_A"
      },
      "source": [
        "# Dataset SyntheticControl No:70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnpHdLFXqESf",
        "outputId": "f38e7e75-7d05-4f74-a4a7-4067cfec6a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training-find.py:26: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 04:47:04,910][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 04:47:04,912][__main__][INFO] - dataset ID: 70, dataset name: SyntheticControl\n",
            "[2023-11-15 04:47:04,912][__main__][INFO] - Number of training + validation data: 300\n",
            "[2023-11-15 04:47:04,912][__main__][INFO] - Length of data: 60\n",
            "[2023-11-15 04:47:08,124][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 04:47:08,125][__main__][INFO] - Length of val_loader: 246\n",
            "[2023-11-15 04:47:08,125][__main__][INFO] - Batch size: 32\n",
            "[2023-11-15 04:47:08,140][__main__][INFO] - data: 2023.11.15.13.47.08\n",
            "[2023-11-15 04:47:08,140][__main__][INFO] - save_name: _2023.11.15.13.47.08_lr_0.0001\n",
            "data_shape [1, 60, 1]\n",
            "path_shape [1, 60, 60]\n",
            "tried power = 0\n",
            "check point\n",
            "data_shape [2, 60, 1]\n",
            "path_shape [2, 60, 60]\n",
            "tried power = 1\n",
            "check point\n",
            "data_shape [4, 60, 1]\n",
            "path_shape [4, 60, 60]\n",
            "tried power = 2\n",
            "check point\n",
            "data_shape [8, 60, 1]\n",
            "path_shape [8, 60, 60]\n",
            "tried power = 3\n",
            "check point\n",
            "data_shape [16, 60, 1]\n",
            "path_shape [16, 60, 60]\n",
            "tried power = 4\n",
            "check point\n",
            "data_shape [32, 60, 1]\n",
            "path_shape [32, 60, 60]\n",
            "tried power = 5\n",
            "check point\n",
            "data_shape [64, 60, 1]\n",
            "path_shape [64, 60, 60]\n",
            "tried power = 6\n",
            "check point\n",
            "data_shape [128, 60, 1]\n",
            "path_shape [128, 60, 60]\n",
            "tried power = 7\n",
            "check point\n",
            "data_shape [256, 60, 1]\n",
            "path_shape [256, 60, 60]\n",
            "tried power = 8\n",
            "check point\n",
            "data_shape [512, 60, 1]\n",
            "path_shape [512, 60, 60]\n",
            "tried power = 9\n",
            "check point\n",
            "data_shape [1024, 60, 1]\n",
            "path_shape [1024, 60, 60]\n",
            "tried power = 10\n",
            "check point\n",
            "CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 440.12 MiB is free. Process 660415 has 15.34 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 936.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "power:  9\n",
            "CPU times: user 105 ms, sys: 11.9 ms, total: 117 ms\n",
            "Wall time: 13.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training-find.py dataset.ID=70 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLOJORdtqESg"
      },
      "outputs": [],
      "source": [
        "power_batch_size = 2**9\n",
        "power_batch_size = 820\n",
        "change_batch_size(power_batch_size,0)\n",
        "change_batch_size(power_batch_size,1) # I guess, metric learning should use roughly the same amount of Vram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy1Bvc3yqESg",
        "outputId": "0308f10b-38d9-4ebc-b863-f8d3c3e42b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-pre-training.py:30: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='pre_training')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 12:50:22,742][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 12:50:22,746][__main__][INFO] - dataset ID: 70, dataset name: SyntheticControl\n",
            "[2023-11-15 12:50:22,746][__main__][INFO] - Number of training + validation data: 300\n",
            "[2023-11-15 12:50:22,747][__main__][INFO] - Length of data: 60\n",
            "[2023-11-15 12:50:26,386][__main__][INFO] - Length of train_loader: 100\n",
            "[2023-11-15 12:50:26,386][__main__][INFO] - Length of val_loader: 10\n",
            "[2023-11-15 12:50:26,386][__main__][INFO] - Batch size: 820\n",
            "[2023-11-15 12:50:26,399][__main__][INFO] - data: 2023.11.15.21.50.26\n",
            "[2023-11-15 12:50:26,399][__main__][INFO] - save_name: _2023.11.15.21.50.26_lr_0.0001\n",
            "100% 100/100 [00:40<00:00,  2.47it/s]\n",
            "100% 10/10 [00:03<00:00,  3.12it/s]\n",
            "[2023-11-15 12:51:11,604][__main__][INFO] - [1/10]-ptime: 43.77, train loss: 0.0108, val loss: 0.0099\n",
            "100% 100/100 [00:40<00:00,  2.45it/s]\n",
            "100% 10/10 [00:04<00:00,  2.30it/s]\n",
            "[2023-11-15 12:51:58,065][__main__][INFO] - [2/10]-ptime: 45.20, train loss: 0.0091, val loss: 0.0094\n",
            "100% 100/100 [00:40<00:00,  2.45it/s]\n",
            "100% 10/10 [00:02<00:00,  3.64it/s]\n",
            "[2023-11-15 12:52:42,421][__main__][INFO] - [3/10]-ptime: 43.53, train loss: 0.0083, val loss: 0.0094\n",
            "100% 100/100 [00:44<00:00,  2.26it/s]\n",
            "100% 10/10 [00:02<00:00,  3.49it/s]\n",
            "[2023-11-15 12:53:30,220][__main__][INFO] - [4/10]-ptime: 47.19, train loss: 0.0074, val loss: 0.0094\n",
            "100% 100/100 [00:41<00:00,  2.40it/s]\n",
            "100% 10/10 [00:03<00:00,  2.62it/s]\n",
            "[2023-11-15 12:54:16,698][__main__][INFO] - [5/10]-ptime: 45.55, train loss: 0.0064, val loss: 0.0097\n",
            "100% 100/100 [00:41<00:00,  2.40it/s]\n",
            "100% 10/10 [00:03<00:00,  2.76it/s]\n",
            "[2023-11-15 12:55:02,540][__main__][INFO] - [6/10]-ptime: 45.28, train loss: 0.0055, val loss: 0.0099\n",
            "100% 100/100 [00:40<00:00,  2.45it/s]\n",
            "100% 10/10 [00:02<00:00,  3.43it/s]\n",
            "[2023-11-15 12:55:46,828][__main__][INFO] - [7/10]-ptime: 43.71, train loss: 0.0047, val loss: 0.0102\n",
            "100% 100/100 [00:41<00:00,  2.41it/s]\n",
            "100% 10/10 [00:04<00:00,  2.37it/s]\n",
            "[2023-11-15 12:56:33,476][__main__][INFO] - [8/10]-ptime: 45.68, train loss: 0.0039, val loss: 0.0102\n",
            "100% 100/100 [00:41<00:00,  2.38it/s]\n",
            "100% 10/10 [00:02<00:00,  3.40it/s]\n",
            "[2023-11-15 12:57:19,083][__main__][INFO] - [9/10]-ptime: 44.93, train loss: 0.0033, val loss: 0.0103\n",
            "100% 100/100 [00:41<00:00,  2.44it/s]\n",
            "100% 10/10 [00:02<00:00,  3.44it/s]\n",
            "[2023-11-15 12:58:03,601][__main__][INFO] - [10/10]-ptime: 43.93, train loss: 0.0028, val loss: 0.0107\n",
            "CPU times: user 4.48 s, sys: 547 ms, total: 5.03 s\n",
            "Wall time: 7min 48s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-pre-training.py dataset.ID=70 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkny0xKpB4lM"
      },
      "outputs": [],
      "source": [
        "%rm -r /content/deep-attentive-time-warping/result/070_SyntheticControl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L769qxjwvvz1",
        "outputId": "2c79c511-71f0-47a3-cc90-3849c0eee5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-attentive-time-warping/code/16-main-metric-learning.py:32: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @ hydra.main(config_path='conf', config_name='metric_learning')\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-11-15 12:58:10,839][__main__][INFO] - \n",
            "=============================================================\n",
            "[2023-11-15 12:58:10,841][__main__][INFO] - dataset ID: 70, dataset name: SyntheticControl\n",
            "[2023-11-15 12:58:10,841][__main__][INFO] - Number of training + validation data: 300\n",
            "[2023-11-15 12:58:10,841][__main__][INFO] - Length of data: 60\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 60, 60]               --\n",
            "├─UNet: 1-1                                        [1, 1, 60, 60]            --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 60, 60]           --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 60, 60]           38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 30, 30]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 30, 30]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 15, 15]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 15, 15]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 7, 7]            --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 7, 7]            3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 3, 3]           --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 3, 3]           14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 7, 7]            --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 6, 6]            2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 7, 7]            7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 15, 15]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 14, 14]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 15, 15]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 30, 30]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 30, 30]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 30, 30]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 60, 60]           --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 60, 60]           32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 60, 60]           110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 60, 60]            --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 60, 60]            65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 2.79\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 31.05\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 155.22\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ProposedModel                                      [1, 60, 60]               --\n",
            "├─UNet: 1-1                                        [1, 1, 60, 60]            --\n",
            "│    └─DoubleConv: 2-1                             [1, 64, 60, 60]           --\n",
            "│    │    └─Sequential: 3-1                        [1, 64, 60, 60]           38,400\n",
            "│    └─Down: 2-2                                   [1, 128, 30, 30]          --\n",
            "│    │    └─Sequential: 3-2                        [1, 128, 30, 30]          221,952\n",
            "│    └─Down: 2-3                                   [1, 256, 15, 15]          --\n",
            "│    │    └─Sequential: 3-3                        [1, 256, 15, 15]          886,272\n",
            "│    └─Down: 2-4                                   [1, 512, 7, 7]            --\n",
            "│    │    └─Sequential: 3-4                        [1, 512, 7, 7]            3,542,016\n",
            "│    └─Down: 2-5                                   [1, 1024, 3, 3]           --\n",
            "│    │    └─Sequential: 3-5                        [1, 1024, 3, 3]           14,161,920\n",
            "│    └─Up: 2-6                                     [1, 512, 7, 7]            --\n",
            "│    │    └─ConvTranspose2d: 3-6                   [1, 512, 6, 6]            2,097,664\n",
            "│    │    └─DoubleConv: 3-7                        [1, 512, 7, 7]            7,080,960\n",
            "│    └─Up: 2-7                                     [1, 256, 15, 15]          --\n",
            "│    │    └─ConvTranspose2d: 3-8                   [1, 256, 14, 14]          524,544\n",
            "│    │    └─DoubleConv: 3-9                        [1, 256, 15, 15]          1,771,008\n",
            "│    └─Up: 2-8                                     [1, 128, 30, 30]          --\n",
            "│    │    └─ConvTranspose2d: 3-10                  [1, 128, 30, 30]          131,200\n",
            "│    │    └─DoubleConv: 3-11                       [1, 128, 30, 30]          443,136\n",
            "│    └─Up: 2-9                                     [1, 64, 60, 60]           --\n",
            "│    │    └─ConvTranspose2d: 3-12                  [1, 64, 60, 60]           32,832\n",
            "│    │    └─DoubleConv: 3-13                       [1, 64, 60, 60]           110,976\n",
            "│    └─OutConv: 2-10                               [1, 1, 60, 60]            --\n",
            "│    │    └─Conv2d: 3-14                           [1, 1, 60, 60]            65\n",
            "====================================================================================================\n",
            "Total params: 31,042,945\n",
            "Trainable params: 31,042,945\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 2.79\n",
            "====================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 31.05\n",
            "Params size (MB): 124.17\n",
            "Estimated Total Size (MB): 155.22\n",
            "====================================================================================================\n",
            "[2023-11-15 12:58:14,090][__main__][INFO] - pre-trained model loading...\n",
            "[2023-11-15 12:58:14,090][__main__][INFO] - pre-trained model: /content/deep-attentive-time-warping//result/070_SyntheticControl/pre_training/SyntheticControl_2023.11.15.21.50.26_lr_0.0001_ProposedModel_epoch_3_loss_0.0094.pkl\n",
            "[2023-11-15 12:58:14,334][__main__][INFO] - Length of train_loader: 500\n",
            "[2023-11-15 12:58:14,334][__main__][INFO] - Batch size: 820\n",
            "[2023-11-15 12:58:14,353][__main__][INFO] - data: 2023.11.15.21.58.14\n",
            "[2023-11-15 12:58:14,353][__main__][INFO] - save_name: _2023.11.15.21.58.14_lr_0.0001\n",
            "100% 500/500 [02:47<00:00,  2.99it/s]\n",
            "100% 29/29 [00:07<00:00,  3.89it/s]\n",
            "[2023-11-15 13:01:10,786][__main__][INFO] - [1/20]-ptime: 174.55, train loss: 0.0163, val loss: 0.0016, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.01it/s]\n",
            "100% 29/29 [00:07<00:00,  3.89it/s]\n",
            "[2023-11-15 13:04:05,919][__main__][INFO] - [2/20]-ptime: 173.76, train loss: 0.0035, val loss: 0.0013, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.82it/s]\n",
            "[2023-11-15 13:07:01,326][__main__][INFO] - [3/20]-ptime: 174.03, train loss: 0.0023, val loss: 0.0011, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.98it/s]\n",
            "[2023-11-15 13:09:57,370][__main__][INFO] - [4/20]-ptime: 173.83, train loss: 0.0019, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.84it/s]\n",
            "[2023-11-15 13:12:53,619][__main__][INFO] - [5/20]-ptime: 174.31, train loss: 0.0156, val loss: 0.0023, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:05<00:00,  5.16it/s]\n",
            "[2023-11-15 13:15:48,119][__main__][INFO] - [6/20]-ptime: 172.39, train loss: 0.0022, val loss: 0.0013, val ER: 0.0000\n",
            "100% 500/500 [02:47<00:00,  2.99it/s]\n",
            "100% 29/29 [00:05<00:00,  5.02it/s]\n",
            "[2023-11-15 13:18:43,747][__main__][INFO] - [7/20]-ptime: 173.27, train loss: 0.0023, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:06<00:00,  4.38it/s]\n",
            "[2023-11-15 13:21:38,820][__main__][INFO] - [8/20]-ptime: 173.40, train loss: 0.0019, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:47<00:00,  2.99it/s]\n",
            "100% 29/29 [00:05<00:00,  5.06it/s]\n",
            "[2023-11-15 13:24:33,060][__main__][INFO] - [9/20]-ptime: 172.77, train loss: 0.0058, val loss: 0.0011, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:05<00:00,  5.02it/s]\n",
            "[2023-11-15 13:27:26,725][__main__][INFO] - [10/20]-ptime: 172.31, train loss: 0.0037, val loss: 0.0014, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  4.03it/s]\n",
            "[2023-11-15 13:30:21,928][__main__][INFO] - [11/20]-ptime: 173.78, train loss: 0.0021, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.99it/s]\n",
            "[2023-11-15 13:33:17,147][__main__][INFO] - [12/20]-ptime: 173.83, train loss: 0.0018, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.83it/s]\n",
            "[2023-11-15 13:36:12,752][__main__][INFO] - [13/20]-ptime: 174.24, train loss: 0.0017, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:08<00:00,  3.47it/s]\n",
            "[2023-11-15 13:39:09,455][__main__][INFO] - [14/20]-ptime: 175.16, train loss: 0.0030, val loss: 0.0019, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.76it/s]\n",
            "[2023-11-15 13:42:05,174][__main__][INFO] - [15/20]-ptime: 174.33, train loss: 0.0018, val loss: 0.0012, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.67it/s]\n",
            "[2023-11-15 13:45:01,214][__main__][INFO] - [16/20]-ptime: 174.59, train loss: 0.0017, val loss: 0.0011, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.73it/s]\n",
            "[2023-11-15 13:47:57,138][__main__][INFO] - [17/20]-ptime: 174.48, train loss: 0.0016, val loss: 0.0012, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:08<00:00,  3.55it/s]\n",
            "[2023-11-15 13:50:53,678][__main__][INFO] - [18/20]-ptime: 175.00, train loss: 0.0028, val loss: 0.0014, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  3.91it/s]\n",
            "[2023-11-15 13:53:49,745][__main__][INFO] - [19/20]-ptime: 174.15, train loss: 0.0016, val loss: 0.0010, val ER: 0.0000\n",
            "100% 500/500 [02:46<00:00,  3.00it/s]\n",
            "100% 29/29 [00:07<00:00,  4.12it/s]\n",
            "[2023-11-15 13:56:45,927][__main__][INFO] - [20/20]-ptime: 173.96, train loss: 0.0017, val loss: 0.0011, val ER: 0.0000\n",
            "[2023-11-15 13:56:45,928][__main__][INFO] - test model loading...\n",
            "[2023-11-15 13:56:45,928][__main__][INFO] - test model: /content/deep-attentive-time-warping//result/070_SyntheticControl/metric_learning/SyntheticControl_2023.11.15.21.58.14_lr_0.0001_ProposedModel_epoch_20_ER_0.0000.pkl\n",
            "100% 300/300 [01:12<00:00,  4.13it/s]\n",
            "[2023-11-15 13:57:58,726][__main__][INFO] - test loss: 0.0014, test ER: 0.0000\n",
            "CPU times: user 36.4 s, sys: 3.84 s, total: 40.2 s\n",
            "Wall time: 59min 54s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!python3 code/16-main-metric-learning.py dataset.ID=70 dataset_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xDDES0R9HLt"
      },
      "source": [
        "# Final saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCF8ZW0y9JJt"
      },
      "outputs": [],
      "source": [
        "%cp /content/deep-attentive-time-warping/result/018_DistalPhalanxTW/metric_learning /content/drive/MyDrive/bilinear_result/result/018_DistalPhalanxTW -r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CqfmPLiS0K8"
      },
      "outputs": [],
      "source": [
        "%cp /content/deep-attentive-time-warping/result/018_DistalPhalanxTW/pre_training /content/drive/MyDrive/bilinear_result/result/018_DistalPhalanxTW -r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmh-EiFfS_EX"
      },
      "outputs": [],
      "source": [
        "%cp /content/deep-attentive-time-warping/result/020_ECG200/metric_learning /content/drive/MyDrive/bilinear_result/result/020_ECG200 -r\n",
        "%cp /content/deep-attentive-time-warping/result/020_ECG200/pre_training /content/drive/MyDrive/bilinear_result/result/020_ECG200 -r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REswHglKTIpQ"
      },
      "outputs": [],
      "source": [
        "%cp -r /content/deep-attentive-time-warping/result/070_SyntheticControl/metric_learning /content/drive/MyDrive/bilinear_result/result/070_SyntheticControl\n",
        "%cp -r /content/deep-attentive-time-warping/result/070_SyntheticControl/pre_training /content/drive/MyDrive/bilinear_result/result/070_SyntheticControl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
